---
title: "final_project"
author: "Amelia Meyer"
date: "6/3/2020"
output: pdf_document
---

```{r}
# adding the new dataset without the missing values
adult <- read.csv('CensusAdult_df.csv', header = T)
names(adult)
attach(adult)
```

Names of the variables:

```{r}
first.mod<-lm(capital.gain~age+workclass+worked	+fnlwgt	+ education	+ marital.status	+ occupation	+ relationship	+ race	+ sex	+ capital.loss	+ hours.per.week	+ native.country	+ income, data = adult)
efirst <- resid(first.mod)
yhatfirst <- fitted(first.mod)

# well-behaved residuals plot with minor fanning pattern
plot(yhatfirst, efirst, xlab = 'Fitted Values', ylab = 'Residuals')
abline(h = 0, lty = 2)

#normality 
qqnorm(efirst)
qqline(efirst)

```


We chose our response value as capital.gain

Optimal model based on forward selection with AIC: capital.gain ~ income + fnlwgt + capital.loss + workclass + age + relationship + hours.per.week

Our predictor values:
x1:income
x2:fnlwgt
x3:capital.loss
x4:workclass
x5:age
x6:relationship
x7:hours.per.week
```{r}
# Finding predictors (refer to days 24 and 26)
# Using forward selection (bottom up method) with AIC
mod.0 <- lm(capital.gain ~ 1, data = adult) 
mod.full <- lm(capital.gain ~., data = adult) # including all predictors in lm()
step(mod.0, scope = list(lower = mod.0, upper = mod.full), direction = "forward") #Uses AIC by default
```

Optimal model using forward selection with BIC:
capital.gain~income + fnlwgt + capital.loss + workclass + age

Our predictors:
x1:income
x2:fnlwgt
x3:capital.loss
x4:workclass
x5:age
```{r}
# Finding predictors (see days 24 and 26)
# Using forward selection (bottom up method) with BIC
## step() function will still print out "AIC", but in fact it is using BIC
n <- length(adult$capital.gain) # sample size
step(mod.0, scope = list(lower = mod.0, upper = mod.full), direction = 'forward',k = log(n), trace = 0)
```

Optimal model using backward selection with AIC:
capital.gain~age + workclass + fnlwgt + relationship + capital.loss + hours.per.week + income

Our predictors:
x1:age
x2:workclass
x3:fnlwgt
x4:relationship
x5:capital.loss
x6:hours.per.week
x7:income

```{r}
# Finding predictors
# Using backward selection (top down method) with AIC
step(mod.full, scope = list(lower = mod.0, upper = mod.full), direction = 'backward')
```

Optimal model using backward selection with BIC:
capital.gain ~ age + workclass + fnlwgt + capital.loss + income

Our predictors: 
x1: age
x2: workclass
x3: fnlwgt
x4: capital.loss
x5: income

```{r}
# Finding predictors
# Using backward selection (top down method) with BIC
step(mod.full, scope = list(lower = mod.0, upper = mod.full), direction = 'backward',k = log(n), trace = 0)
```

Optimal model using stepwise selection:
capital.gain ~ age + workclass + fnlwgt + relationship + capital.loss + hours.per.week + income

Our predictors:
x1: age
x2: workclass
x3: fnlwgt
x4: relationship
x5: capital.loss
x6: hours.per.week
x7: income

```{r}
# Finding predictors
# Using stepwise selection (hybrid model)
step(mod.0, scope = list(lower = mod.0, upper = mod.full))
step(mod.full, scope = list(lower = mod.0, upper = mod.full))
```

Optimal model using regsubsets:
Based on rsq, our model should contain 2 predictors. Based on adjr2, our model should contain 8 predictors. Based on Cp, our model should contain 7 predictors. Based on BIC, our model should contain 6 predictors. We need to do further analysis in order to determine whether the 7th and 8th predictors are helpful. 

```{r}
library(leaps)
mod.reg <- regsubsets(cbind(age,workclass,worked,fnlwgt,education,marital.status,occupation,relationship,race,sex,capital.loss,hours.per.week,native.country,income),capital.gain, data=adult)
summary.reg <- summary(mod.reg)
names(summary.reg)
summary.reg$which
summary.reg$rsq
summary.reg$adjr2
summary.reg$cp
summary.reg$bic

par(mfrow = c(2, 2))
plot(summary.reg$rsq, xlab = "Number of Variables", ylab = "RSq", type = "b")

plot(summary.reg$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "b")
best_adj_r2 = which.max(summary.reg$adjr2)
points(best_adj_r2, summary.reg$adjr2[best_adj_r2],
       col = "red",cex = 2, pch = 20)

plot(summary.reg$cp, xlab = "Number of Variables", ylab = "Cp", type = 'b')
best_cp = which.min(summary.reg$cp[-c(length(summary.reg$cp))])
points(best_cp, summary.reg$cp[best_cp],
       col = "red", cex = 2, pch = 20)

plot(summary.reg$bic, xlab = "Number of Variables", ylab = "BIC", type = 'b')
best_bic = which.min(summary.reg$bic)
points(best_bic, summary.reg$bic[best_bic],
       col = "red", cex = 2, pch = 20)

mod.full <- lm(capital.gain ~., data = adult) 
mod.red <- lm(capital.gain ~income + fnlwgt + capital.loss + workclass + age)
anova(mod.red, mod.full)
```

Marital.status is not a usefule predictor

```{r}
# checking to see if predictors 'marital.status' and 'relationship' are necessary for our model

# Running test for B7=0 aka marital.status=0
# Added Varaible Plots
full.lm <- lm(capital.gain ~ income + fnlwgt + capital.loss + workclass + age + marital.status)
# avPlots(full.lm, id=FALSE)

# summary table 
summary(full.lm)

# Global F-test, $H_0: \beta_1 = \beta_2 = \beta_3 = 0$
red.lm <- lm(capital.gain ~ 1)
anova(red.lm, full.lm)

# Global F-test, $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ ( ? )
anova(full.lm, red.lm)
```

Relationship is not a useful predictor

```{r}
# Running test for B8=0 aka relationship
# First figure out the output of B7=0
# Added Variable Plots
full2.lm <- lm(capital.gain ~ income + fnlwgt + capital.loss + workclass + age + relationship) 
# we know marital.status would go before relationship because that was in the output of the regsubsets function
# avPlots(full2.lm, id=FALSE)

# summary table 
summary(full2.lm)

# Global F-test, $H_0: \beta_1 = \beta_2 = \beta_3 = 0$
red.lm <- lm(capital.gain ~ 1)
anova(red.lm, full2.lm)

# Global F-test, $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ ( ? )
anova(full2.lm, red.lm)
```

Look at the plots for the current model, before transformation:
```{r}
second.mod<-lm(capital.gain~income+capital.loss+education+age+workclass+hours.per.week,data=adult)
esecond <- resid(second.mod)
yhatsecond <- fitted(second.mod)

# well-behaved residuals plot with minor fanning pattern
plot(yhatsecond, esecond, xlab = 'Fitted Values', ylab = 'Residuals')
abline(h = 0, lty = 2)

#normality seems okay
qqnorm(esecond)
qqline(esecond)
```

Power transformation using adjusted values for adult data

Rd powers
capital.loss: -1.74 
education: 1.34
age: 0.13
hours.per.week: 0.95 

```{r}
# Transforming predictors using 1 + capital.loss
library(car)

capital.loss01<-with(adult,capital.loss+0.01)
Trans.adult<-powerTransform(cbind(capital.loss01,education,age,hours.per.week)~1, adult)
summary(Trans.adult)


testTransform(Trans.adult, lambda = c(1, 1, 1, 0))
Adult.Trans<-with(adult,data.frame(capital.loss01^(-1.74),education^(1.34),age^0.11,hours.per.week^0.95))
pairs(Adult.Trans)
```

Now that we have our model (excluding categorical variables):

capital.gain ~ capital.loss01^(-1.74)+education^(1.34)+age^(0.11)+hours.per.week^(.95)

```{r}
# Transforming the response

adult$capital.gain01<-with(adult,capital.gain+0.01)
Adult.power<-powerTransform(capital.gain01~.,Adult.Trans)
summary(Adult.power)

library(MASS)
adult.lm<-lm(capital.gain01~.,data=Adult.Trans)
boxcox(adult.lm)
```

We would transform capital.gain to the power of -0.88
So our current model is looking like:

capital.gain01^(-0.88) ~ income + capital.loss01^(-1.74) + education^(1.34) + age^(0.13) + workclass + hours.per.week^(0.95) 
before entering the categorical indicator variables

Looking at the plots for the transformed response and predictors

Model slightly more condensed with Normal Q-Q Plot

```{r}

capital.gain1<-capital.gain01^(-0.88)
capital.loss1<-capital.loss01^(-1.74)
education1<-education^(1.34)
age1<-age^(0.11)
hours.per.week1<-hours.per.week^(0.95)

aftrans.mod<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1	+ workclass + hours.per.week1	, data = adult)
eaftrans <- resid(aftrans.mod)
yhataftrans <- fitted(aftrans.mod)

# well-behaved residuals plot with minor fanning pattern
plot(yhataftrans, eaftrans, xlab = 'Fitted Values', ylab = 'Residuals')
abline(h = 0, lty = 2)

#normality seems okay
qqnorm(eaftrans)
qqline(eaftrans)

```

Indicator variables

```{r}
# caterogical variables:
# income, k-1 = 1 indicators variable x1 (k-> <=50K, or >50K)
# workclass, k-1 = 4, (k -> State-gov, Self-emp-not-inc, Private, Federal-gov, Local-gov) x2, x3, x4, x5

capital.gain1<-capital.gain01^(-0.88)
capital.loss1<-capital.loss01^(-1.74)
education1<-education^(1.34)
age1<-age^(0.11)
hours.per.week1<-hours.per.week^(0.95)

factor.lm <- lm(capital.gain1 ~ income) 
summary(factor.lm)

factor1.lm <- lm(capital.gain1 ~ workclass)
summary(factor1.lm)
```

Fit the proper model

```{r}
# Nonparallel
np.lm <- lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1, data = adult)
```

Now we need to do a partial F test to make sure we sure add our terms

```{r}
## Partial F Tests
### Nonparallel vs. Parallel

summary(np.lm)

#equivalent to the partial F test
null.lm <- lm(capital.gain1 ~ 1, data = adult) # Intercept only
anova(null.lm, np.lm)

# We still would reject our null hyp
# At least one of the predictors matters

# if we removed all indicator variables
par.lm <- lm(capital.gain1 ~ income + capital.loss1 + education1 +age1 + age1 + workclass + hours.per.week1, data = adult) 
anova(par.lm, np.lm) # always do the smaller model first

# Non parallel model is preferred to parallel model

summary(par.lm)

```

Since we're using the nonparaller model, we need to figure out which predictors can be taken out of the following model:

capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1

H0: B7=0 vs H1: B7!=0
p value is 1.233e-10 *** so reject null, income:capital.loss1 is a useful predictor

```{r}
# Is income:capital.loss1 a useful predictor given that the other predictors are in the model?
full.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1)

redB7.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1)
anova(redB7.lm,full.lm)

```

H0: B8=0 vs B8!=0
p value is 2.2e-16 *** so income:education1 is a useful predictor

```{r}
# Is income:education1 a useful predictor given that the other predictors are in the model?
redB8.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1)
anova(redB8.lm, full.lm)
```

H0: B9=0 vs H1: B9!=0
p value is 6.22e-16 *** so income:age1 is a useful predictor

```{r}
redB9.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1)
anova(redB9.lm, full.lm)
```

H0: B10=0 vs H1: B10!=0
p value is 1.233e-10 *** so income:hours.per.week1 is a useful predictor

```{r}
redB10.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1)
anova(redB10.lm,full.lm)
```

H0: B11=0 vs H1: B11!=0
p value is 0.002405 ** so workclass:capital.loss1 is a useful predictor for alpha = 0.05, but it's not a usefule predictor for alpha=0.001

```{r}
redB11.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1)
anova(redB11.lm, full.lm)
```

H0: B12=0 vs H1: B12!=0
p value is 2.2e-16 *** so workclass:education1 is a useful predictor

```{r}
redB12.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:age1 + workclass:hours.per.week1)
anova(redB12.lm, full.lm)
```

H0: B13=0 vs H1: B13!=0
p value is 1.215e-05 *** so workclass:age1 is a useful predictor

```{r}
redB13.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:hours.per.week1)
anova(redB13.lm,full.lm)
```

H0: B14=0 vs H1: B14!=0
p value is 0.9253 so workkclass:hours.per.week1 is not a useful predictor

```{r}
redB14.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1)
anova(redB14.lm,full.lm)
```

Look at scatterplotMatrix to check the marginal relationships

```{r}
scatterplotMatrix(~capital.gain1 + income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1)
```

Diagnostic assumptions:

```{r}
# Residual analysis (Model Evaluation)

final.mod <- lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1, data = adult)
e <- resid(final.mod)
yhat <- fitted(final.mod)

# well-behaved residuals plot with minor fanning pattern
plot(yhat, e, xlab = 'Fitted Values', ylab = 'Residuals')
abline(h = 0, lty = 2)

#normality seems okay
qqnorm(e)
qqline(e)

#large p-value: residuals are normal
# shapiro.test(e)
# can't do because dample size exceeds 5000
ncvTest(~capital.gain1 + income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1)

## Interaction term
int.mod <- lm(capital.gain1 ~ income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 , data = adult)

#no interaction effect exists
anova(final.mod, int.mod)
```

Checking for outliers (unusual y)

```{r}
#find studentized residuals
fit.all <- lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1, data=adult)
(rst <-  rstudent(fit.all))
#find the point with the largest studentized residual
rst <- abs(rst)
which(rst > 3); which(rst > 2)

#find standardized residuals
fit1.all <- lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1, data=adult)
rstandard(fit1.all)

#find the point with the largest standardized residual
rs <- abs(rstandard(fit1.all))
which(rs == max(rs))
```

Checking for high leverage points (unusual x)

```{r}
#find leverages
## Cook's Distance
fit2.all<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1, data=adult)
ds <- cooks.distance(fit2.all)
round(ds,5)
which(ds > 4/(30163-13-1))
```

When we did the calculation for the outliers and the high leverage points we got thousands of points which means that they aren't in fact outliers or high leverage points, they're just representative of the spread out trend of our data. The points seen as outliers and high leverage don't stay from the general trend of our data, therefore they are imporant and should not be removed. 


Questions we're trying to answer:
```{r}
# 1 What is the predicted value of capital gain when capital loss is 1000? 

#Confidence interval for Mean Response
fullmod.lm<-lm(capital.gain1 ~ income + capital.loss1 + education1 + age1 + workclass + hours.per.week1 + income:capital.loss1 + income:education1 + income:age1 + income:hours.per.week1 + workclass:capital.loss1 + workclass:education1 + workclass:age1 + workclass:hours.per.week1)

clmean <- mean(capital.loss1)
edumean <- mean(education1)
agemean <- mean(age1)
hpwmean <- mean(hours.per.week1)
iclmean <-  mean(income:capital.loss1)
iedumean <- mean(income:education1)
iagemean <- mean(income:age1)
iinmean <- mean(income:hours.per.week1)
wccmean <- mean(workclass:capital.loss1)
wcedumean <- mean(workclass:education1)
wclagemean <- mean(workclass:age1)
wchpwmean <- mean(workclass:hours.per.week1)
(new <- data.frame(income, clmean, edumean,agemean,hpwmean,iclmean,iedumean,iagemean,iinmean,wccmean,wcedumean,wclagemean,wchpwmean))

predict(fullmod.lm, new, interval = 'confidence', level = 0.95)

# 2 Is there a strong relationship between education and capital gain?
capital.gain1_vs_edu.lm<-lm(capital.gain1~education1)
summary(capital.gain1_vs_edu.lm)
# adjusted rsq is 0.03212 so very very small positive relationship

# 3 Is there a strong relationship between hours per week and capital gain? 
capital.gain1_vs_hours.per.week1.lm<-lm(capital.gain1~hours.per.week1)
summary(capital.gain1_vs_hours.per.week1.lm)
# adjusted rsq is 0.009176 so very small positive relationship

# 5 Is there a significant difference in the capital gain at age 18 and age 40 using a Prediction interval for a new response

age.lm <- lm(capital.gain1 ~ age1)
# Expected capital.gain at age 18
predict(age.lm , data.frame(age1 = 18 ),
        interval = 'prediction', level = 0.95)
# Expected capital.gain at 40
predict(age.lm , data.frame(age = 40),
        interval = 'prediction', level = 0.95)


```

